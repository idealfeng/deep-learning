import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc

# --- é…ç½® ---
base_model_path = "Qwen/Qwen1.5-1.8B-Chat"
# ã€é‡è¦ã€‘ç¡®ä¿è¿™é‡Œçš„è·¯å¾„æ˜¯ä½ æœ€æ–°è®­ç»ƒçš„ "jiangjian" æ¨¡å‹çš„è·¯å¾„
lora_path = "./girl_qwen_chat_1.8b"

# --- åŠ è½½åˆ†è¯å™¨ ---è®© transformers èƒ½æ­£ç¡®åŠ è½½ Qwen è‡ªå®šä¹‰ä»£ç é€»è¾‘ã€‚
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)


# --- å®šä¹‰ç”Ÿæˆå‡½æ•° ---
def generate_response(model, instruction):
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": instruction}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)      # ç”¨ apply_chat_template() è‡ªåŠ¨ç”Ÿæˆ Qwen é£æ ¼çš„ promptï¼ˆå¸¦ <|im_start|>user ç­‰æ ‡è®°ï¼‰
    model_inputs = tokenizer([text], return_tensors="pt").to("cuda")        # tokenizer([text], return_tensors="pt") â†’ è½¬æˆ tensorã€‚å¹¶é€å…¥gpu

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=100,     # æœ€å¤šç”Ÿæˆ 100 ä¸ªæ–° tokenã€‚
        temperature=0.9  # è®©æˆ‘ä»¬ç”¨é‚£ä¸ªæ•ˆæœæœ€å¥½çš„â€œé»„é‡‘æ¸©åº¦â€
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response


# --- é—®é¢˜åˆ—è¡¨ ---
test_prompts = [
    "è€ƒç ”æŠ¥è¿‡åäº†ï¼Ÿ",
    "ä»Šå¤©è‹±è¯­å•è¯èƒŒäº†å—ï¼Ÿ",
    "è¯„ä»·ä¸€ä¸‹æˆ‘ä»Šå¤©çš„ç©¿æ­ã€‚",
    "æˆ‘çœ‹æ˜Ÿç©¹é“é“ç­–åˆ’æ˜¯æƒ³æ­»äº†",
    "å¦‚ä½•è¯„ä»·ä½ å­¦æ ¡è¢«ç§°ä¸ºâ€œä¸Šæµ·å°æ¸…åâ€ï¼Ÿ",
    "ä½ å¯¹ç™½å„æ€ä¹ˆçœ‹ï¼Ÿ"
]

# --- å­˜å‚¨ç»“æœçš„å­—å…¸ ---
results = {prompt: {} for prompt in test_prompts}

# --- ä¸¥æ ¼åˆ†ç¦»çš„æµ‹è¯•æµç¨‹ ---

# 1. åŠ è½½å¹¶æµ‹è¯•çº¯å‡€çš„åŸå§‹æ¨¡å‹
print("=" * 50)
print("æ­£åœ¨åŠ è½½å’Œæµ‹è¯• ğŸ¤– çº¯å‡€çš„åŸå§‹æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,     # ä½¿ç”¨ bfloat16 å‡å°‘æ˜¾å­˜å ç”¨ã€‚
    device_map="auto",
    trust_remote_code=True
)
for prompt in test_prompts:
    response = generate_response(base_model, prompt)
    results[prompt]["base"] = response

# 2. ã€æ ¸å¿ƒã€‘åœ¨æµ‹è¯•å®ŒåŸå§‹æ¨¡å‹åï¼Œå†è¿›è¡Œâ€œæ”¹è£…â€
print("\næ­£åœ¨å°†LoRAé€‚é…å™¨åº”ç”¨åˆ°æ¨¡å‹ä¸Š...")
tuned_model = PeftModel.from_pretrained(base_model, lora_path)      # æŠŠ LoRA å‚æ•°æ³¨å…¥åˆ°åŸæ¨¡å‹ï¼Œå¾—åˆ°ä¸€ä¸ªæ”¹è£…æ¨¡å‹ã€‚
print("æ¨¡å‹æ”¹è£…å®Œæˆï¼")

# 3. æµ‹è¯•æ”¹è£…åçš„æ¨¡å‹
print("\næ­£åœ¨æµ‹è¯• ğŸ§‘â€ğŸ¨ æ‚¨çš„å¾®è°ƒæ¨¡å‹...")
for prompt in test_prompts:
    response = generate_response(tuned_model, prompt)
    results[prompt]["tuned"] = response

# --- å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜ ---
del base_model
del tuned_model
gc.collect()
torch.cuda.empty_cache()

# --- æœ€ç»ˆç»“æœå¯¹æ¯”å±•ç¤º ---
print("\n\n" + "=" * 25 + " æœ€ç»ˆå¯¹æ¯” " + "=" * 25)
for prompt in test_prompts:
    print("\n" + "-" * 50)
    print(f"ğŸ¤” é—®é¢˜: {prompt}")
    print("-" * 50)
    print(f"ğŸ¤– åŸå§‹æ¨¡å‹çš„å›ç­”:\n{results[prompt]['base']}")
    print(f"\nğŸ§‘â€ğŸ¨ ä½ çš„æ¨¡å‹çš„å›ç­”:\n{results[prompt]['tuned']}")

print("\n" + "=" * 50)
print("æ‰€æœ‰å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")