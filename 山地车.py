import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import time
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

# ==================== 1. DQN ç½‘ç»œ ====================
class DQN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(2, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3)
        )

    def forward(self, x):
        return self.fc(x)


# ==================== 2. ç»éªŒå›æ”¾ ====================
class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


# ==================== 3. è®­ç»ƒé…ç½® ====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

env = gym.make('MountainCar-v0')

policy_net = DQN().to(device)
target_net = DQN().to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.Adam(policy_net.parameters(), lr=0.001)
memory = ReplayBuffer(capacity=10000)

# è¶…å‚æ•°
epsilon = 1.0
epsilon_decay = 0.995
min_epsilon = 0.01
gamma = 0.99
batch_size = 64
target_update = 10

# ==================== 4. è®°å½•è®­ç»ƒæ•°æ® ====================
episode_rewards = []
episode_steps = []
success_episodes = []
epsilon_history = []

# ==================== 5. è®­ç»ƒè¿‡ç¨‹ ====================
print("=" * 50)
print("å¼€å§‹ DQN è®­ç»ƒ...")
print("=" * 50)

num_episodes = 500
success_count = 0

for episode in range(num_episodes):
    state, _ = env.reset()
    total_reward = 0

    for step in range(200):
        # Epsilon-greedy ç­–ç•¥
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                action = policy_net(state_tensor).argmax().item()

        next_state, reward, done, truncated, _ = env.step(action)

        # è½»å¾®çš„å¥–åŠ±å¡‘é€ 
        position, velocity = next_state
        shaped_reward = reward + abs(velocity) * 5

        memory.push(state, action, shaped_reward, next_state, done)
        state = next_state
        total_reward += reward

        # è®­ç»ƒç½‘ç»œ
        if len(memory) >= batch_size:
            batch = memory.sample(batch_size)
            states, actions, rewards, next_states, dones = zip(*batch)

            states = torch.FloatTensor(states).to(device)
            actions = torch.LongTensor(actions).to(device)
            rewards = torch.FloatTensor(rewards).to(device)
            next_states = torch.FloatTensor(next_states).to(device)
            dones = torch.FloatTensor(dones).to(device)

            # Q-learning æ›´æ–°
            current_q = policy_net(states).gather(1, actions.unsqueeze(1))
            next_q = target_net(next_states).max(1)[0].detach()
            target_q = rewards + gamma * next_q * (1 - dones)

            loss = nn.MSELoss()(current_q.squeeze(), target_q)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if done:
            success_count += 1
            success_episodes.append(episode)
            break

    # è®°å½•æ•°æ®
    episode_rewards.append(total_reward)
    episode_steps.append(step + 1)
    epsilon_history.append(epsilon)

    # è¡°å‡ epsilon
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    # æ›´æ–°ç›®æ ‡ç½‘ç»œ
    if episode % target_update == 0:
        target_net.load_state_dict(policy_net.state_dict())

    # æ‰“å°è¿›åº¦
    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(episode_rewards[-50:])
        avg_steps = np.mean(episode_steps[-50:])
        recent_success = sum(1 for e in success_episodes if e >= episode - 49)
        print(f"Episode {episode + 1}/{num_episodes} | "
              f"æˆåŠŸç‡: {recent_success}/50 | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:.1f} | "
              f"å¹³å‡æ­¥æ•°: {avg_steps:.1f} | "
              f"Îµ: {epsilon:.3f}")

print("\n" + "=" * 50)
print(f"è®­ç»ƒå®Œæˆï¼æ€»æˆåŠŸæ¬¡æ•°: {success_count}/{num_episodes}")
print("=" * 50)

# ==================== 6. ç»˜åˆ¶è®­ç»ƒæ›²çº¿ ====================
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('DQN è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–', fontsize=16, fontweight='bold')

# å­å›¾1: æ¯è½®å¥–åŠ±
axes[0, 0].plot(episode_rewards, alpha=0.6, label='æ¯è½®å¥–åŠ±')
axes[0, 0].plot(np.convolve(episode_rewards, np.ones(50) / 50, mode='valid'),
                'r-', linewidth=2, label='50è½®å‡å€¼')
axes[0, 0].set_xlabel('Episode')
axes[0, 0].set_ylabel('Total Reward')
axes[0, 0].set_title('å¥–åŠ±æ›²çº¿')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# å­å›¾2: æ¯è½®æ­¥æ•°
axes[0, 1].plot(episode_steps, alpha=0.6, label='æ¯è½®æ­¥æ•°')
axes[0, 1].plot(np.convolve(episode_steps, np.ones(50) / 50, mode='valid'),
                'g-', linewidth=2, label='50è½®å‡å€¼')
axes[0, 1].axhline(y=110, color='r', linestyle='--', label='äººç±»æ°´å¹³')
axes[0, 1].set_xlabel('Episode')
axes[0, 1].set_ylabel('Steps')
axes[0, 1].set_title('æ­¥æ•°æ›²çº¿ï¼ˆè¶Šä½è¶Šå¥½ï¼‰')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# å­å›¾3: æˆåŠŸç‡ç»Ÿè®¡
window = 50
success_rate = []
for i in range(len(success_episodes)):
    if i < window:
        rate = len([e for e in success_episodes if e <= i]) / (i + 1) * 100
    else:
        rate = len([e for e in success_episodes if e > i - window and e <= i]) / window * 100
    success_rate.append(rate)

if success_rate:
    axes[1, 0].plot(success_rate, 'b-', linewidth=2)
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Success Rate (%)')
    axes[1, 0].set_title(f'æˆåŠŸç‡ï¼ˆæ»‘åŠ¨çª—å£={window}ï¼‰')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_ylim([0, 105])

# å­å›¾4: Epsilon è¡°å‡
axes[1, 1].plot(epsilon_history, 'purple', linewidth=2)
axes[1, 1].set_xlabel('Episode')
axes[1, 1].set_ylabel('Epsilon')
axes[1, 1].set_title('æ¢ç´¢ç‡è¡°å‡')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('dqn_training_results.png', dpi=300, bbox_inches='tight')
print("\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º 'dqn_training_results.png'")
plt.show()

# ==================== 7. ä¿å­˜æ¨¡å‹ ====================
torch.save({
    'policy_net': policy_net.state_dict(),
    'target_net': target_net.state_dict(),
    'optimizer': optimizer.state_dict(),
}, 'mountaincar_dqn.pth')
print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º 'mountaincar_dqn.pth'")

# ==================== 8. æµ‹è¯•é˜¶æ®µ ====================
print("\n" + "=" * 50)
print("å¼€å§‹æµ‹è¯•ï¼ˆå¸¦å¯è§†åŒ–ï¼‰...")
print("=" * 50)

env.close()
env = gym.make('MountainCar-v0', render_mode='human')

for test_num in range(3):
    print(f"\nğŸ® æµ‹è¯• {test_num + 1}/3:")
    state, _ = env.reset()
    total_reward = 0

    for step in range(300):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            action = policy_net(state_tensor).argmax().item()

        state, reward, done, truncated, _ = env.step(action)
        total_reward += reward

        time.sleep(0.02)  # æ”¾æ…¢é€Ÿåº¦ä¾¿äºè§‚å¯Ÿ

        if done:
            print(f"âœ… æˆåŠŸå†²é¡¶ï¼ç”¨æ—¶ {step + 1} æ­¥ï¼Œæ€»å¥–åŠ±: {total_reward:.0f}")
            time.sleep(1)
            break
    else:
        print(f"âŒ æœªæˆåŠŸï¼Œç”¨æ—¶ {step + 1} æ­¥ï¼Œæ€»å¥–åŠ±: {total_reward:.0f}")

env.close()
print("\n" + "=" * 50)
print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
print("=" * 50)