import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ä¸€ä¸ªå¸¸ç”¨çš„ä¸­æ–‡ç”µå•†è¯„è®ºäºŒåˆ†ç±»æƒ…æ„Ÿæ¨¡å‹ï¼ˆHuggingFaceï¼‰
# æ ‡ç­¾é€šå¸¸æ˜¯ï¼š0=negative, 1=positive
MODEL_NAME = "uer/roberta-base-finetuned-jd-binary-chinese"

def pick_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")

@torch.no_grad()
def predict(texts, tokenizer, model, device, max_len=128):
    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_len,
        return_tensors="pt",
    ).to(device)

    logits = model(**enc).logits
    probs = torch.softmax(logits, dim=-1)

    # probs[..., 1] = positive prob
    results = []
    for i, t in enumerate(texts):
        p_neg = probs[i, 0].item()
        p_pos = probs[i, 1].item()
        label = "æ­£å‘ğŸ‘" if p_pos >= p_neg else "è´Ÿå‘ğŸ‘"
        results.append((t, label, p_pos, p_neg))
    return results

def token_saliency(text, tokenizer, model, device, max_len=128, topk=12):
    """
    ä¸€ä¸ªâ€œå¤Ÿç”¨ä¸”ç›´è§‚â€çš„è§£é‡Šï¼šå¯¹é¢„æµ‹ç±»åˆ«logitæ±‚ embedding æ¢¯åº¦ï¼Œ
    ç”¨ |grad * emb| çš„ L2 èŒƒæ•°å½“ä½œæ¯ä¸ª token çš„é‡è¦æ€§åˆ†æ•°ã€‚
    """
    model.eval()

    enc = tokenizer(
        text,
        truncation=True,
        max_length=max_len,
        return_tensors="pt",
    )
    input_ids = enc["input_ids"].to(device)
    attention_mask = enc["attention_mask"].to(device)

    # å– base model çš„è¯å‘é‡å±‚ (é€‚é… BERT/RoBERTa è¿™ç±»ç»“æ„)
    base = getattr(model, "bert", None) or getattr(model, "roberta", None) or model.base_model
    emb_layer = base.embeddings.word_embeddings

    # (1) å…ˆæŠŠ input_ids -> embeddingsï¼Œå¹¶è®©å®ƒå¯æ±‚å¯¼
    inputs_embeds = emb_layer(input_ids)
    inputs_embeds.requires_grad_(True)  # å…³é”®ï¼šç¡®ä¿å¯ç”¨æ¢¯åº¦

    # åœ¨è¿™é‡Œï¼Œæ˜¾å¼å‘Šè¯‰ PyTorch ä¿ç•™æ¢¯åº¦
    inputs_embeds.retain_grad()  # ä¿ç•™æ¢¯åº¦

    # (2) ç”¨ inputs_embeds å‰å‘ï¼ˆå¤šæ•° BERT/RoBERTa ç»“æ„æ”¯æŒï¼‰
    out = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)
    logits = out.logits[0]
    pred = int(torch.argmax(logits).item())

    # (3) åå‘ï¼šå¯¹é¢„æµ‹ç±»åˆ« logit æ±‚æ¢¯åº¦
    model.zero_grad(set_to_none=True)
    logits[pred].backward()

    grads = inputs_embeds.grad[0]              # [seq_len, hidden]
    embs  = inputs_embeds.detach()[0]          # [seq_len, hidden]
    scores = (grads * embs).abs().norm(p=2, dim=-1)  # [seq_len]

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())

    # å»æ‰ç‰¹æ®Š token
    pairs = []
    for tok, sc in zip(tokens, scores.tolist()):
        if tok in ["[CLS]", "[SEP]", "[PAD]"]:
            continue
        pairs.append((tok, sc))

    pairs.sort(key=lambda x: x[1], reverse=True)
    top = pairs[:topk]

    # å‹å¥½æ‰“å°ï¼šæŠŠ WordPiece å‰ç¼€ ## åˆå¹¶æ˜¾ç¤ºå¾—æ›´åƒâ€œè¯â€
    def pretty(tok):
        return tok.replace("##", "")

    return pred, [(pretty(t), float(s)) for t, s in top]


def main():
    device = pick_device()
    print(f"Using device: {device}")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)

    demo_texts = [
        "è¿™æ‰‹æœºç»­èˆªå¤ªé¡¶äº†ï¼Œå±å¹•ä¹Ÿå¾ˆèˆ’æœï¼ŒçœŸé¦™ï¼",
        "å®¢æœæ€åº¦å¾ˆå·®ï¼Œç‰©æµæ…¢å¾—ç¦»è°±ï¼Œå¤ªå¤±æœ›äº†ã€‚",
        "ä¸€èˆ¬èˆ¬å§ï¼Œæ²¡æƒ³è±¡ä¸­å¥½ï¼Œä½†ä¹Ÿä¸ç®—å·®ã€‚",
        "åšå·¥å¾ˆå·®ï¼ŒæŒ‰é”®æ¾åŠ¨ï¼Œè¿˜å‘çƒ­ï¼Œåˆ«ä¹°ã€‚",
        "åŒ…è£…ç²¾è‡´ï¼Œæ‰‹æ„Ÿä¸é”™ï¼Œæ€§ä»·æ¯”å¾ˆé«˜ã€‚",
        "æˆ‘åœ¨ç½‘ä¸Šæœäº†åŠå¤©éƒ½æ²¡æ‰¾åˆ°ï¼Œé—®aié—®å‡ºæ¥äº†,åˆ°åº•æƒ³å¹²å•¥",
        "æ€ä¹ˆè¿˜å¤šäº†1.12,æ”¶äº†äº”åˆ†é’Ÿçš„åˆ©æ¯",
        "æˆ‘å®¤å‹æ‰“æ¸¸æˆè¿éº¦åˆ°ä¸¤ç‚¹,ä»–ä¹Ÿè€ƒç ”,å“ˆå“ˆ,æˆ‘æƒ³ä¸‹å»é“œä¸ä»–",
        "ä½ çŸ¥é“å—,åˆ‡æ¯”é›ªå¤«ä¸ç­‰å¼é‡Œè—ç€äº²äº²ğŸ˜˜,æ¯å¤©éƒ½æœ‰æ–°å‘ç°",
        "æˆ‘çœŸæƒ³ç©é¸£æ½®äº†,æœ‰ç©ºä¹°ä¸ªå·",
        "ä½ æ²¡å­¦ç¬¬å››ç« ,å¿«å­¦,å­¦äº†å°±èƒ½åš24å¹´ç¬¬åé¢˜äº†",
        "è¿™å®¶ä¼™åœ¨è¯´ä»€ä¹ˆå‘¢,çœŸä¼šæ±‚å—ï¼Ÿæ¥ä¸ªå½“åœºç»ƒä¹ ",
        "æˆ‘æ˜¯çœŸå¾—ç»™ä½ å…æ‰“æ‰°äº†,ç®—äº†æˆ‘ç»™æ‰‹æœºå¼€ä¸ªå…æ‰“æ‰°å§",
        "æƒŠå¤©å¤§op,è¦æ˜¯æˆ‘ä¸€å¤©ä¸­æœ€æƒ¬æ„çš„æ—¶åˆ»æ˜¯æ‰“å¼€åŸç¥æˆ‘çœ‹æˆ‘å¾—åœ¨6æ¥¼æ”»å‡»æ°´æ³¥åœ°é¢äº†",
        "ä½ çœ‹çœ‹å¤ªå¥½ç©äº†",
        "æ•´ä¸ªå‡‰çš®è‚‰å¤¹é¦åƒåƒ,è¿˜æœ‰å†°å³°",
        "è€äº†ä¸‰å¹´å¯¼è‡´çš„,æˆ‘æ­£å„¿å…«ç»è€äº†ä¸‰å¹´,æœ‰ç‚¹æ”¾çºµäº†,å±äºæ˜¯å›é€†æœŸå»¶é•¿åˆ°å¤§å­¦æ¥äº†,ä½†ç°åœ¨æˆ‘è¿‡å»äº†"
    ]

    print("\n=== æƒ…æ„Ÿé¢„æµ‹ ===")
    results = predict(demo_texts, tokenizer, model, device)
    for t, label, p_pos, p_neg in results:
        print(f"- {t}\n  é¢„æµ‹: {label} | P(pos)={p_pos:.3f}, P(neg)={p_neg:.3f}")

    # é€‰ä¸€ä¸ªå¥å­çœ‹â€œæ¨¡å‹ä¸»è¦ç›¯ç€å“ªäº› tokenâ€
    text = "å®¢æœæ€åº¦å¾ˆå·®ï¼Œç‰©æµæ…¢å¾—ç¦»è°±ï¼Œå¤ªå¤±æœ›äº†ã€‚"
    pred, top = token_saliency(text, tokenizer, model, device)
    pred_label = "æ­£å‘ğŸ‘" if pred == 1 else "è´Ÿå‘ğŸ‘"

    print("\n=== ç®€æ˜“è§£é‡Šï¼ˆtoken é‡è¦æ€§ Topï¼‰ ===")
    print(f"å¥å­: {text}")
    print(f"é¢„æµ‹: {pred_label}")
    for tok, sc in top:
        print(f"  {tok:>6s}  score={sc:.4f}")

    print("\nä½ ä¹Ÿå¯ä»¥æ”¹ demo_textsï¼Œæ”¾è¿›å»ä½ è‡ªå·±çš„å¥å­è¯•è¯•ã€‚")

if __name__ == "__main__":
    main()
